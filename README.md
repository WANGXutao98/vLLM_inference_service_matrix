# vLLMæ¨ç†æ¡†æ¶

åŸºäºè¿‡å»å·¥ä½œä¸­å®ç°çš„åŸºäºvLLM Inference Frameworkï¼Œç›®å‰å·²å°†ä»£ç æ¡†æ¶éƒ¨åˆ†æŠ½è±¡ï¼Œä»¥æ”¯æŒå¤šç§åè®®å’Œè‡ªå®šä¹‰æ‰©å±•ã€‚

## ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½**: åŸºäºvLLMçš„é«˜æ•ˆæ¨ç†å¼•æ“
- ğŸ”Œ **å¤šåè®®æ”¯æŒ**: OpenAI Chat APIã€Completion APIå’Œè‡ªå®šä¹‰åè®®
- ğŸ›¡ï¸ **å®‰å…¨å¯é **: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œå®‰å…¨éªŒè¯
- ğŸ“Š **ç›‘æ§å®Œå¤‡**: å†…ç½®æŒ‡æ ‡æ”¶é›†å’Œæ€§èƒ½ç›‘æ§
- ğŸ”§ **æ˜“äºæ‰©å±•**: æ’ä»¶åŒ–æ¶æ„ï¼Œæ”¯æŒè‡ªå®šä¹‰å¤„ç†å™¨
- ğŸ³ **å®¹å™¨åŒ–**: å®Œæ•´çš„Dockeræ”¯æŒ

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- CUDA 11.8+ (GPUæ¨ç†)
- 8GB+ GPUå†…å­˜

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone <repository-url>
cd vLLM-Inference-Framework

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# é…ç½®æ¨¡å‹
cp conf/model_config.json.example conf/model_config.json
# ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œè®¾ç½®æ¨¡å‹è·¯å¾„å’Œå‚æ•°
```

### è¿è¡ŒæœåŠ¡

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®å¯åŠ¨
python service/vllm_infer_service.py --model-name your_model_name

# è‡ªå®šä¹‰é…ç½®å¯åŠ¨
python service/vllm_infer_service.py \
    --model-name your_model_name \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 1
```

### Dockerè¿è¡Œ

```bash
# æ„å»ºé•œåƒ
docker build -t vllm-inference .

# è¿è¡Œå®¹å™¨
docker run -d \
    --gpus all \
    -p 8000:8000 \
    -v /path/to/models:/models \
    vllm-inference \
    --model-name your_model_name
```

## APIä½¿ç”¨

### Chat API

```bash
curl -X POST "http://localhost:8000/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your_model",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'
```

### Completion API

```bash
curl -X POST "http://localhost:8000/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your_model",
    "prompt": "Complete this sentence: The future of AI is",
    "temperature": 0.7,
    "max_tokens": 100
  }'
```

### Custom API

```bash
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your_model",
    "prompt": "Your custom prompt",
    "system_info": "System instructions",
    "history_input": ["Previous input"],
    "history_output": ["Previous output"],
    "temperature": 0.7,
    "max_tokens": 100
  }'
```

## é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®

ç¼–è¾‘ `conf/model_config.json`:

```json
{
  "your_model_name": {
    "class_name": "OpenAIServingChat",
    "model_path": "/path/to/your/model",
    "max_model_len": 4096,
    "chat_template": "/path/to/chat_template.jinja",
    "response_role": "assistant"
  }
}
```

### ç¯å¢ƒå˜é‡

```bash
# æœåŠ¡é…ç½®
export SERVICE_PORT=8000
export MODEL_NAME=your_model_name

# PolarisæœåŠ¡å‘ç°
# Rainbowé…ç½®ä¸­å¿ƒ
```

## æ¶æ„è®¾è®¡

### æ ¸å¿ƒç»„ä»¶

- **ServingEngine**: åŸºç¡€æœåŠ¡å¼•æ“æŠ½è±¡
- **ResponseGenerator**: ä¼˜åŒ–çš„å“åº”ç”Ÿæˆå™¨
- **ErrorHandler**: ç»Ÿä¸€é”™è¯¯å¤„ç†
- **MetricsReporter**: æŒ‡æ ‡æ”¶é›†å’ŒæŠ¥å‘Š

### åè®®æ”¯æŒ

- **Chat Protocol**: OpenAI Chat APIå…¼å®¹
- **Completion Protocol**: OpenAI Completion APIå…¼å®¹  
- **Custom Protocol**: è‡ªå®šä¹‰åè®®ï¼Œæ”¯æŒå†å²å¯¹è¯

### æ‰©å±•æœºåˆ¶

æ¡†æ¶æ”¯æŒé€šè¿‡æ’ä»¶ç³»ç»Ÿè¿›è¡Œæ‰©å±•:

```python
from service.core import BaseServingEngine

class CustomServingEngine(BaseServingEngine):
    def create_prompt(self, request, raw_request):
        # è‡ªå®šä¹‰æç¤ºè¯å¤„ç†
        return custom_prompt
    
    def create_error_response(self, message, **kwargs):
        # è‡ªå®šä¹‰é”™è¯¯å“åº”
        return custom_error_response
```

## ç›‘æ§å’Œæ—¥å¿—

### æŒ‡æ ‡ç›‘æ§

æ¡†æ¶è‡ªåŠ¨æ”¶é›†ä»¥ä¸‹æŒ‡æ ‡:

- è¯·æ±‚æ€»æ•°å’ŒæˆåŠŸç‡
- å“åº”æ—¶é—´å’Œé¦–åŒ…æ—¶é—´
- Tokenä½¿ç”¨ç»Ÿè®¡
- é”™è¯¯ç‡ç»Ÿè®¡

### æ—¥å¿—é…ç½®

æ—¥å¿—æ”¯æŒå¤šç§è¾“å‡ºæ–¹å¼:

- æ§åˆ¶å°è¾“å‡º
- æ–‡ä»¶è¾“å‡º
- è¿œç¨‹æ—¥å¿—æœåŠ¡ (æ”¯æŒæ™ºçœ¼æ—¥å¿—)

## æ€§èƒ½ä¼˜åŒ–

### å·²å®ç°çš„ä¼˜åŒ–

- âœ… å­—ç¬¦ä¸²æ“ä½œä¼˜åŒ–
- âœ… å“åº”ç¼“å­˜æœºåˆ¶
- âœ… å¼‚æ­¥æ–‡ä»¶æ“ä½œ
- âœ… å†…å­˜ä½¿ç”¨ä¼˜åŒ–

### å»ºè®®é…ç½®

- ä½¿ç”¨GPUæ¨ç†: `--tensor-parallel-size 1`
- è°ƒæ•´å¹¶å‘æ•°: `--limit-concurrency 100`
- ä¼˜åŒ–å†…å­˜: `--max-model-len 4096`

## å®‰å…¨ç‰¹æ€§

- âœ… è¾“å…¥éªŒè¯å’Œè·¯å¾„éå†é˜²æŠ¤
- âœ… åŠ¨æ€ç±»åŠ è½½ç™½åå•
- âœ… æ•æ„Ÿä¿¡æ¯ç¯å¢ƒå˜é‡åŒ–
- âœ… é”™è¯¯ä¿¡æ¯è„±æ•

## å¼€å‘æŒ‡å—

### ä»£ç è§„èŒƒ

- ä½¿ç”¨Python 3.8+ç±»å‹æ³¨è§£
- éµå¾ªPEP 8ä»£ç é£æ ¼
- æ·»åŠ å®Œæ•´çš„æ–‡æ¡£å­—ç¬¦ä¸²
- ç¼–å†™å•å…ƒæµ‹è¯•

### è´¡çŒ®æµç¨‹

1. Forké¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
3. æäº¤ä»£ç å¹¶æ·»åŠ æµ‹è¯•
4. åˆ›å»ºPull Request

### æµ‹è¯•

```bash
# è¿è¡Œå•å…ƒæµ‹è¯•
python -m pytest tests/

# è¿è¡Œé›†æˆæµ‹è¯•
python -m pytest tests/integration/

# æ€§èƒ½æµ‹è¯•
python tools/benchmark.py
```


### v1.0.0
- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒå¤šç§åè®®
- åŸºç¡€ç›‘æ§å’Œæ—¥å¿—åŠŸèƒ½
- Dockeræ”¯æŒ